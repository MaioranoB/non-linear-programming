{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COS360 - Otimização 2020.2\n",
    "## Bernardo Maiorano e Igor Amaral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\displaystyle \\min \\ \\  &f(x)\\\\\n",
    "\\text{sujeito a:}\\  \\ \\ &x \\in \\Omega\\\\\n",
    "\\\\\n",
    "f(x_1,x_2,x_3,x_4) &= -30x_1 - 10x_2 - 40x_3 - 12x_4\\\\\n",
    "\\Omega = \\{x \\in \\mathbb{R}^4 :& \\  {33x_1 + 14x_2 + 47x_3 + 11x_4 \\le 59\\}} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) fazer um estudo da função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = np.array([-30, -10, -40, -12])\n",
    "omega = np.array([33, 14, 47, 11])\n",
    "\n",
    "def f(x):\n",
    "    return x @ coefficients\n",
    "\n",
    "def G(x):\n",
    "    # Restrição de desigualdade\n",
    "    return omega @ x <= 59\n",
    "\n",
    "# Gradientes constantes\n",
    "grad_f = coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64.36363636363636"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mínimo pelo simplex: x = [0,0,0,59/11]\n",
    "f([0,0,0,59/11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalidade Exterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para transformar nosso problema restrito em irrestrito. Temos que usar o método de penalidade externa ou interna.\n",
    "$ \\\\ $\n",
    "Assim, o problema transformado seria: $\\min \\phi(x,\\rho) = f(x) + \\rho P(x)$ onde $\\rho$ é o parâmetro de penalidade e $P(x)$ é a função de penalidade.\n",
    "\n",
    "Vamos usar a função de penalidade externa: $\\displaystyle P(x) = \\sum_{h \\in H} h(x)^2 + \\sum_{g \\in G} \\max[0, g(x)]^2$\n",
    "\n",
    "No nosso caso, a função objetivo torna-se: $ \\\\ $\n",
    "$$\n",
    "{-30x_1-10x_2-40x_3-12x_4} + \\rho \\max[0,{33x_1+14x_2+47x_3+11x_4-59}]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_Exterior(x, p = 1000):\n",
    "    return f(x) + p*pow(max(0, omega@x-59),2)\n",
    "\n",
    "# https://www.wolframalpha.com/input/?i=derivate+%28p*%28max+%280%2C+%2833x%2B14y%2B47z%2B11w-59%29%29%29%5E2\n",
    "# grad_Pext = grad_f + p*grad(Penalidade)\n",
    "def grad_Pext(x, p= 1000):\n",
    "    if G(x): return grad_f\n",
    "    return np.array([(grad_f[i] + (p*2*omega[i]*(x@omega-59))) for i in range(4)])\n",
    "\n",
    "# https://www.wolframalpha.com/input/?i=hessian+%28%28-30x-10y-40z-12w%29+%2B+%28p*%2833x%2B14y%2B47z%2B11w-59%29%5E2%29%29+with+respect+to+%28x%2Cy%2Cz%2Cw%29\n",
    "def hessian_Pext(x, p= 1000):\n",
    "    hessian = np.zeros((4,4))\n",
    "    if G(x): return hessian\n",
    "    for i in range(4):\n",
    "        hessian[i] = [(p*2*omega[i]*omega[j]) for j in range(4)]\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([-30, -10, -40, -12]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([0,0,0,0])\n",
    "P_Exterior(z), grad_Pext(z)#, hessian_Pext(k,1), np.linalg.inv(hessian_Pext(k,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalidade Interior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de penalidade interior: f(x) - p/G(x)\n",
    "def P_Interior(x, p = 0.0001):\n",
    "    # p: parâmetro de penalidade\n",
    "    return f(x) - (p/(omega@x-59))\n",
    "\n",
    "# https://www.wolframalpha.com/input/?i=derivate+%28%28-30x-10y-40z-12w%29-%28p%2F%2833x%2B14y%2B47z%2B11w-59%29%29%29\n",
    "# grad_Pin = grad_f - p*grad(Penalidade)\n",
    "def grad_Pin(x, p = 0.0001):\n",
    "    return np.array([(grad_f[i] - p*(-omega[i]/pow(omega@x-59, 2))) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6949152542372882e-06,\n",
       " array([-29.99999905,  -9.9999996 , -39.99999865, -11.99999968]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_Interior(z), grad_Pin(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(x, direction, f, Grad):\n",
    "    '''\n",
    "    '''\n",
    "    t = 1\n",
    "    gamma = 0.8\n",
    "    n = 0.25\n",
    "    n_iter = 0\n",
    "    \n",
    "    while (f(x + t*direction) > f(x) + n*t*Grad(x)@direction):\n",
    "        t = t*gamma\n",
    "        n_iter += 1\n",
    "        \n",
    "#     print(\"Armijo:\",t,n_iter)\n",
    "    return t, n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop(x, prev_x, n_iter, max_iters = 50):\n",
    "    return np.allclose(x, prev_x, atol=1e-16) or n_iter > max_iters\n",
    " \n",
    "def Gradiente(x, f, Grad):\n",
    "    \n",
    "    #ponto inicial fora do viável\n",
    "    if not G(x):\n",
    "        print('fora do viável')\n",
    "        return\n",
    "    \n",
    "    k = 0\n",
    "    n_iter = 0\n",
    "    prev_x = float('inf')*np.ones(4)\n",
    "    while not stop(x,prev_x,n_iter):\n",
    "        direction = -Grad(x)\n",
    "        t,_ = armijo(x,direction, f, Grad)\n",
    "        prev_x = x\n",
    "        x = x + t*direction\n",
    "        k += 1\n",
    "        n_iter += 1\n",
    "#         print(f(x),x)\n",
    "    return x, f(x), n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.56334641, 0.18776653, 0.75111295, 0.22535001]),\n",
       " -51.52667222900208,\n",
       " 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0,0,0,0], dtype=np.float64)\n",
    "Gradiente(x, P_Exterior, grad_Pext)\n",
    "# Gradiente(x, P_Interior, grad_Pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Newton(x, f, Grad, Hessian):\n",
    "#     k = 0\n",
    "#     n_iter = 0\n",
    "#     prev_x = float('inf')*np.ones(4)\n",
    "#     while not stop(x,prev_x,n_iter):\n",
    "#         direction = -(inverse(Hessian)@Grad(x))\n",
    "\n",
    "# def quase_Newton(x,f,Grad,Hessian):\n",
    "#     k = 0\n",
    "#     n_iter = 0\n",
    "#     prev_x = float('inf')*np.ones(4)\n",
    "#     while not stop(x,prev_x,n_iter):\n",
    "#         direction = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
